{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35012721-dbd0-4b67-9710-e59fa1d2946c",
   "metadata": {},
   "source": [
    "# Multitask learning\n",
    "\n",
    "Hydranet in Tesla is a multitask learning example. It is extremely hard to train the beast and let multiple teams to work in the same large network.\n",
    "\n",
    "As a subset of the beast network, prediction and behavior are naturally to be trained together, since many of self driving engineers (including me) believes these two tasks are deeply corelated and can be solved more efficiently when jointly modeling them. The general concept is to encode the environment by some backbone network (e.g. resnet or densenet) and then have multiple heads for prediction and decision tasks.\n",
    "\n",
    "This ends up with the so-called hard sharing network.\n",
    "\n",
    "![](../assets/multitask/multitask.png)\n",
    "\n",
    "This network is difficult to train well due to:\n",
    "\n",
    "1. There are bunch of heads to be either classification or regression tasks. Some are easier, some are harder. Balance of training resource is critical to reach better minimal.\n",
    "2. The dataset can be in different scale and noise level for each task.\n",
    "3. In practice, usually the baseline is the single task network. It means you can have a good baseline of prediction or decision task. However, you can not simply freeze the baseline as the main network and train the another task as a head. Because sometimes the input domain are not exactly the same. For example, route info is needed in decision task, but it is not required in prediction task. A workaround is to add the route info after the backbone and encode it again in cnn. This is not the elegant solution.\n",
    "\n",
    "The 1st problem usually is tackled from loss design and gradient design. The oversample of small number dataset can push some task's train dataset into the same scale as others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecbdc97-f740-48ab-b818-8fdf6e2d7028",
   "metadata": {},
   "source": [
    "## Weighted sum of loss\n",
    "\n",
    "The naive one is:\n",
    "$$\n",
    "Loss = \\sum w L\n",
    "$$\n",
    "$w$ of each task is manully setted by engineer. Usually the weight is selected to scale each task loss to the same level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a44232-79c9-4aee-8926-19118564aea4",
   "metadata": {},
   "source": [
    "## Uncertainty aware loss\n",
    "\n",
    "https://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf\n",
    "\n",
    "This one should be my default choice of multitask learning loss. It usually performs better than the naive one, at least it doesn't hurt.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0968ab3eeb915af72db7aacf10deaadc03d3b360e8f9cd2458aa8925b160b4c7c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
