---
layout: post
title:  "Benchmark of Popular Reinforcement Learning Algorithms"
date:   2019-01-24 10:00:00
categories: Deep learning
---

This post is a reflection of my study through OpenAI's tutorial, [Spinningup](git@github.com:BingyuZhou/spinningup.git), of deep reinforcement learning. It mainly covers six popular algorithms including **Vanilla Policy Gradient (VPG)**, **Trust Region Policy Optimization (TRPO)**, **Proximal Policy Optimization (PPO)**, **Deep Deterministic Policy Gradient (DDPG)**, **Twin Delayed DDPG (TD3)** and **Soft Actor-Critic (SAC)**. I have implemented all these algorithms under the guidance of the tutorial. I would like to highlight the key ideas and critical theory behind each algorithm in this post. Meanwhile, I will also share the benchmark performance of these algorithms using my own implementation.

The figure below shows the category of all the presentated algorithms. They all belongs to model free RL algorithms, since they do not explicitly learn a model or have access to the environment model.

![image](/assets/drl.png "drl")

Policy Optimization:

- directly optimizes the performance of the policy, either explicitly using policy gradient, or modifying policy gradient for the sake of stability and efficientcy.
- normally belongs to actor-acritic structure
- usually runs in on-policy
- stable and reliable training as *directly optimizing what we want*


Q Learning:

- also called value learning methods;
- implicitly finds optimal policy through learning action value or value function to match Bellman equation. Optimal actions are computed through optimal Q function:
$$ a^* = \arg\max Q^*(s,a) $$

- almost always runs in off-policy
- less stable training in practice
- higher sample efficiency when it works

The mixed algorithms (TD3, SAC, DDPG) lie between these two approches.

### Vanilla Policy Gradient

This algorithm is the basic policy gradient version, which is also called REINFORCE. The concept is quite simple and direct. Aiming to maximize the discounted return 
$$ J = \max_{\tau \sim \pi} E(R(\tau)) $$,
this method explicitly update the policy with the gradient of the cost function. The policy gradient has a beautiful format which can be easily deducted using some basic math 
knowledge. Here is the policy gradient:

$$ g = E_{\tau \sim \pi_\theta} [\sum_{t=0}^{t=T} \nabla \log \pi_\theta(a|s) R(\tau)] $$

which is easy to remember as *Exp-Grad-Log*. This analytical format can be estimated by sampling trajectories at policy $$\pi_\theta $$. It ends up with the numerical calculation format as:
\begin{equation}
\hat{g} = \frac{1}{\mathcal{D}} \sum_{\tau \sim \mathcal{D}} \sum \nabla_\theta \log \pi_{\theta}(a|s) R(\tau)
\label{eq:grad}
\end{equation}

There are two important variations based on \eqref{eq:grad}, which can imporove the learning process.

- Rewards-to-go
\begin{equation}
\hat{g} = \frac{1}{\mathcal{D}} \sum_{\tau \sim \mathcal{D}} \sum \nabla_\theta \log \pi_{\theta}(a|s) \underline{R_t(\tau)}
\label{eq:r-to-go}
\end{equation}

Instead of using the complete return $$R(\tau)$$, we replace it with the rewards-to-go term:

$$
\begin{equation*}
R_t(\tau) = \sum_{t=t'}^{T} r_t,
\end{equation*}
$$

this term makes the policy forget the previous rewards, which benifits it by not stucking at old rewards. 

- Advantage (Highly preferred in practice!)

$$
\begin{equation*}
\hat{g} = \frac{1}{\mathcal{D}} \sum_{\tau \sim \mathcal{D}} \sum \nabla_\theta \log \pi_{\theta}(a|s) \underline{A(s_t, a_t)}
\end{equation*}
$$

Advantage means that how much better it is to take a certain action, over randomly selecting actions on a state. It is a relative comparison compared with the absolute return by accumulating rewards.

$$
\begin{equation}
A(s_t, a_t) = Q(a_t, s_t) - V(s_t).
\label{eq:adv}
\end{equation}
$$

In practice, $$A(s_t, a_t)$$ is not known. [GAE](https://arxiv.org/abs/1506.02438) gives an accurate estimate of the advantage in a nice mathematical formula:

$$
\begin{equation*}
A_t(s_t, a_t) = \sum_{c=0}^{inf} (\gamma \lambda)^c (r_{t+c}+\gamma V(s_{t+c+1}) - V(s_{t+c}))
\end{equation*}
$$

where $$\lambda$$ is close to 1 for the best trade-off of varaince and bias.

There is a fast implementation to compute a sequence of advantage along timesteps,

```python
def cum_discounted_sum(self, x, discount):
        """
        Magic formula to compute cummunated sum of discounted value (Faster)
        Input: x = [x1, x2, x3]
        Output: x1+discount*x2+discount^2*x3, x2+discount*x3, x3
        """
        return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]
```

This is the fastest implementation as I know compared with the other ways, like the forward computation and the diagnal matrix assignment. 



### Trust Region Policy Optimization

TRPO can be regarded as a strengthen version of VPG, which aims to take the maximimal gradient step without driving away too much from the previous policy. A main feature of TRPO is that it optimizes the relative improvement of the policy performance, which is modelled in a surrogate advantage $$J(\theta_k, \theta) = E[\frac{\pi_\theta}{\pi_{\theta_k}} A^{\theta_k}]$$. The updated policy is constraint with the [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence):

$$D_{KL}(\theta || \theta_k) = E(D_{KL}(\pi_\theta(.|s) || \pi_{\theta_k}(.|s))) \leq \delta $$

where $$\theta_k$$ is the old policy parameters.

Using Taylor expansion and Lagrangian duality methods, the analytical solution of the optimization problem is:

$$
\begin{equation}
\theta_{k+1} = \theta_k + \sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1}g
\label{eq:npg}
\end{equation}
$$

where $$H$$ is the hession matrix of $$D_{kl}$$ and $$g$$ is the gradient of $$J$$.

\eqref{eq:npg} is called *Natural Policy Gradient*. However, due to to approximation error of Taylor expansions, this solution can violate the KL-divergence constraint. Therefore, we search the largest step we can take within the constraint $$\alpha^j \sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1}g$$, which is equivalent to find the smallest *j* without breaking the constraint.

It is notorious that computing inverse of hession matrix is expansive and numerically unstable. The [conjugate gradient](https://en.wikipedia.org/wiki/Conjugate_gradient_method) is specifically used to solve this problem. We try to solve $$Hx=g$$, with $$x=H^{-1}g$$. 

```python
conjugate 
```

### Proximal Policy Optimization

### Deep Deterministic Policy Gradient

### Twin Delayed DDPG

### Soft Actor-Critic


